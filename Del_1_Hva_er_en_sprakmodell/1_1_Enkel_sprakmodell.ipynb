{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db558a1-c764-4549-8e12-9290da8fda3e",
   "metadata": {},
   "source": [
    "# 1. Norsk UD-trebank: Et nynorsk tekstdatasett"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a9915-7b1f-4a2b-8f84-6ed556f090b1",
   "metadata": {},
   "source": [
    "# Sammendrag\n",
    "\n",
    "\n",
    "Den norske UD-trebanken er basert på nynorskdelen av den norske\n",
    "Dependency Treebank (NDT), som er en norsk, syntaktisk trebank.\n",
    "NDT er automatisk konvertert til UD-schema av Lilja Øvrelid ved Universitetet i Oslo.\n",
    "\n",
    "\n",
    "# Introduksjon\n",
    "\n",
    "\n",
    "NDT ble utviklet fra 2011 til 2014 ved Nasjonalbiblioteket i samarbeid med Tekstlaboratoriet og Institutt for informatikk ved Universitetet i Oslo.\n",
    "NDT inneholder rundt 300 000 tokens hentet fra en rekke forskjellige sjangre. Trebanktekstene er manuelt annotert for morfosyntaktisk informasjon. Denne annoteringen følger hovedsakelig [Oslo-Bergen Tagger](http://tekstlab.uio.no/obt-ny/). Den syntaktiske annoteringen følger i stor grad Norwegian Reference Grammar, samt et avhengighetsmerkeskjema formulert ved begynnelsen av annoteringsprosjektet og iterativt raffinert gjennom byggingen av trebanken. For mer informasjon se referanser nedenfor.\n",
    "\n",
    "## Kjør koden nedenfor for å laste inn datasettet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ec4b8-da4b-4be4-8957-6479c6206838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_data\n",
    "\n",
    "nynorskCorpus, nynorskTokens = read_data(\"../datasets/UD_Norwegian-Nynorsk-master/no_nynorsk-ud-train.conllu\")\n",
    "# nynorskCorpus, nynorskTokens = read_data(\"../datasets/UD_Norwegian-Nynorsk-master/no_nynorsk-ud-test.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0629088-61c3-449d-9874-6316d801e6a7",
   "metadata": {},
   "source": [
    "## 1.1 Visualiser tekstkorpuset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c883e2c3-f3ce-4831-aa32-5f96f90497f4",
   "metadata": {},
   "source": [
    "La oss se nærmere på ordene som er tilgjengelige i datasettet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dde672-4089-47da-8d40-ec65017fc0b3",
   "metadata": {},
   "source": [
    "### Ordsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b749f9-7a73-4741-82b5-611a238a9e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(\" \".join(nynorskTokens))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Word Cloud\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be242b-20ec-4b2a-b9ed-8c247d85024c",
   "metadata": {},
   "source": [
    "### Ordfrekvens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca09d81-c17a-4c6d-a74b-2158a62d1207",
   "metadata": {},
   "source": [
    "La oss undersøke hvilke ord som er mest vanlige i datasettet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7e4bc-7657-43d6-8fc9-682779fd4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import FreqDist\n",
    "import seaborn as sns\n",
    "\n",
    "freq_dist = FreqDist(nynorskTokens)\n",
    "df_freq_dist = pd.DataFrame(list(freq_dist.items()), columns=[\"Token\", \"Frequency\"])\n",
    "df_freq_dist = df_freq_dist.sort_values(by=\"Frequency\", ascending=False)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Token\", y=\"Frequency\", data=df_freq_dist.head(20))\n",
    "plt.title(\"Top 20 Most Frequent Tokens\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0ec2c-f336-447a-9e10-fcb333cddf22",
   "metadata": {},
   "source": [
    "### Ordlengde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b7b92-4f78-46b6-93cf-a0cc5fb77464",
   "metadata": {},
   "source": [
    "Hvor lange er ordene i datasettet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e912e19-f2bf-4e75-887e-657f9167a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(nynorskTokens)), [len(token) for token in nynorskTokens], alpha=0.5)\n",
    "plt.title(\"Scatter Plot of Word Lengths\")\n",
    "plt.xlabel(\"Token Index\")\n",
    "plt.ylabel(\"Word Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7768a-1bbd-418f-80f0-61c4e9ca7137",
   "metadata": {},
   "source": [
    "# 2. Tren en enkel språkmodell for setningsgenerering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ffc06-8534-4429-b92b-869295d9e40b",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; flex-direction: row;\">\n",
    "    <img src=\"resources/ngram.png\" alt=\"Image 1\" width=\"500px\" height=\"150px\" style=\"margin-left: 300px;\">\n",
    "</div>\n",
    "\n",
    "N-gram-modeller er grunnleggende språkmodeller for tekstgenerering, og gir et sannsynlighets-rammeverk for å beskrive strukturen og mønstrene i en sekvens av ord. I kontekst av NLP er en n-gram en sekvens av n ord, og n-gram-modeller estimerer sannsynligheten for et ord basert på dets kontekst – de foregående n-1 ordene. Antakelsen er at sannsynligheten for et ord bare avhenger av en begrenset historie med foregående ord, noe som gjør beregningen mer gjennomførbar. Disse modellene tilbyr enkelhet og effektivitet, noe som gjør dem grunnleggende i ulike språkbehandlingsoppgaver inkludert tekstgenerering, maskinoversettelse og talegjenkjenning. N-gram-modeller viser effektivitet når det gjelder lokale avhengigheter, men de kan sliter over lengre rekkevidde og mislykkes i å fange opp den bredere semantiske konteksten som finnes i mer avanserte modeller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d641b9e-5cdc-47ee-875d-0b28fe3ef559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a simple Bigram model from the training data loaded above.\n",
    "\n",
    "from utils import train_ngram_model\n",
    "ngram_model = train_ngram_model(nynorskTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30631956905ae116",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate text starting with a seed\n",
    "from utils import generate_text\n",
    "\n",
    "seed = '...' # The initial prompt for generating a sentence.\n",
    "generated_text = generate_text(ngram_model , seed, length=50) # generate sample text using the given function.\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f378c-ec74-42e5-90ba-7214dbf758cc",
   "metadata": {},
   "source": [
    "# 3. Store språkmodeller\n",
    "\n",
    "<div style=\"display: flex; flex-direction: row;\">\n",
    "    <img src=\"resources/lstm.png\" alt=\"Image 1\" width=\"500px\" height=\"150px\" style=\"margin-right: 10px;\">\n",
    "    <img src=\"resources/transformer.png\" alt=\"Image 2\" width=\"500px\" height=\"100\" style=\"margin-right: 10px;\">\n",
    "</div>\n",
    "\n",
    "Språkmodeller basert på nevrale nettverk, slik som recurrent neural networks (RNNs), long short-term memory networks (LSTMs) og transformerarkitekturer, har vist overlegen resultater sammenlignet med n-gram-modeller. Her er noen grunner til at nevrale språkmodeller generelt anses som bedre:\n",
    "\n",
    "***Avhengigheter:***\n",
    "\n",
    "N-gram-modeller fanger avhengigheter opp til et fast antall foregående ord (n i n-gram). Nevrale språkmodeller, spesielt transformerarkitekturer, kan fange opp avhengigheter over en lengre rekkefølge av ord, noe som gjør at de kan modellere mer komplekse sammenhenger.\n",
    "\n",
    "***Parameter-effektivitet:***\n",
    "\n",
    "Nevrale språkmodeller kan effektivt representere og lære fra store mengder data med relativt få parametere sammenlignet med n-gram-modeller. Dette er avgjørende for å håndtere den store mengden informasjon som finnes i naturlig språk.\n",
    "\n",
    "***Kontinuerlig embeddings:***\n",
    "\n",
    "Nevrale modeller representerer ord som kontinuerlige embeddings i et høydimensjonalt rom. Denne kontinuerlige representasjonen gjør at modellen kan fange opp semantiske forhold mellom ord, noe som er utfordrende for diskrete representasjoner brukt i n-gram.\n",
    "\n",
    "***Generalisering:***\n",
    "\n",
    "Nevrale modeller generaliserer bedre til ukjente eller sjeldne ord fordi de lærer kontinuerlige representasjoner som fanger opp likheter mellom ord. N-gram-modeller sliter med ord som ikke finnes i vokabularet og generaliserer dårlig til ukjente kontekster.\n",
    "\n",
    "***Tilpasningsevne:***\n",
    "\n",
    "Nevrale modeller kan tilpasse seg kompleksiteten til forskjellige NLP-oppgaver ved å justere hyperparametere. N-gram-modeller har begrenset kapasitet til å tilpasse seg forskjellige oppgaver uten å endre n-gram-rekkefølgen, noe som ikke er praktisk.\n",
    "\n",
    "***Variabel kontekstlengde:***\n",
    "\n",
    "Nevrale modeller kan håndtere kontekster av variablel lengde, noe som gjør dem mer fleksible i behandlingen av sekvenser med forskjellige lengder. I motsetning til dette krever n-gram-modeller kontekster med fast lengde, noe som kan være begrensende.\n",
    "\n",
    "***Kontekstuell informasjon:***\n",
    "\n",
    "Nevrale modeller som BERT (Bidirectional Encoder Representations from Transformers) og GPT (Generative Pre-trained Transformer) tar hensyn til kontekstuell informasjon ved å behandle hele sekvensen bidireksjonalt eller unidireksjonalt. Dette gjør at de kan fange opp rikere kontekst for hvert ord.\n",
    "\n",
    "***Ytelse:***\n",
    "\n",
    "Nevrale språkmodeller har oppnådd state-of-the-art ytelse på ulike NLP-benchmarktester, inkludert oppgaver som språkmodellering, maskinoversettelse, tekstsummering og sentimentanalyse. Til tross for fordelene med nevrale språkmodeller, kan n-gram-modeller fortsatt være nyttige i visse scenarier, spesielt når en har begrensede ressurser eller når en enkel modell er tilstrekkelig for oppgaven. Valget mellom n-gram-modeller og nevrale språkmodeller avhenger ofte av de spesifikke kravene til oppgaven, mengden tilgjengelige data og ressurser.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0fda7-d2b2-4e6e-bbef-2090ef7d629e",
   "metadata": {},
   "source": [
    "## Tokenisering\n",
    "\n",
    "Sjekk ut denne lenken som viser tokenisering for ChatGPT: https://platform.openai.com/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573cb499-8512-420c-83f5-9ad14c9a55ea",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; flex-direction: row;\">\n",
    "    <img src=\"resources/tokenizer.png\" alt=\"Image 1\" width=\"700px\" height=\"300px\" style=\"margin-left: 200px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ce552-dcc2-467a-8e4e-c758205e557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tokenize_text\n",
    "tokens = tokenize_text(\"Supercalifragilisticexpialidocious\")\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "tokens = tokenize_text(\"ordbokstavrimkonkurranse\")\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "tokens = tokenize_text(\"Enter your word here\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9097f-aabb-48cb-bc72-670214b01467",
   "metadata": {},
   "source": [
    "## Å kjøre en LLM lokalt  er ressurskrevende og tar lang tid. Vi vil derfor i neste avsnitt bruke OpenAIs API for ekstern tilgang til en GPT-modell."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
