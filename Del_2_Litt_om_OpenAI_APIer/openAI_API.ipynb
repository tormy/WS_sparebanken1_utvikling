{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b59266",
   "metadata": {},
   "source": [
    "## 0. Initialisering\n",
    "For at eksemplene i notebooken skal fungere smertefritt, må openai og dotenv (python-dotenv)-bibliotekene være installert i miljøet der du starter notebooken. Om nødvendig kan du kjøre `!pip install <library>` i en kode-celle for å installere et manglende bibliotek. \n",
    "\n",
    "Før vi begynner å gjøre noe interessant, må vi importere noen bibliotek og metoder, og vi må lese inn en gyldig openai-nøkkel (leses her fra en lokal .env - fil)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5307aae-1834-4027-9c19-5a90ffdd6e1f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# bibliotek vi trenger i dette kapittelet\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import json\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# forenklende pandas-setting\n",
    "pd.options.mode.copy_on_write = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89a3aed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# leser API-nøkkel og initialiserer OpenAI-klient\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # leser fra i lokal .env-fil\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "client=OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85065ac-481b-4a06-80e1-243b094f5a0d",
   "metadata": {},
   "source": [
    "## 1. Grunnleggende bruk av ChatCompletions-endepunktet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2607e84a-32d9-4922-8e3a-1835807d0675",
   "metadata": {},
   "source": [
    "### 1.1 Kalle API-et og få en respons...  \n",
    "For å få API-et til å generere en respons på et spørsmål i naturlig språk, må vi...\n",
    " - initialisere klienten\n",
    " - spesifisere hvilken språkmodell vi ønsker å bruke, med \"model\" - parameteren (se [OpenAI model docs](https://platform.openai.com/docs/models/overview) for oppdaterte detaljer)\n",
    " - initialisere \"samtalen\" med \"message\" - parameteren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a4509-8fa1-41cc-aecb-bf8636a32206",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Gjør en første forespørsel til API\n",
    "\n",
    "# forespørsel til modellen gis gjennom \"messages\" -parameteren\n",
    "messages = [  \n",
    "{\"role\": \"system\", \"content\": \"You are a helpful tourist information agent in Oslo, Norway.\"},    \n",
    "{\"role\": \"user\", \"content\": \"Hvor finner jeg det berømte vikingskipsmuseet?\"}  \n",
    "] \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "pprint(response.choices[0].message.content, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00f4484-46fc-44ab-9daf-31800be71201",
   "metadata": {},
   "source": [
    "### 1.2 Noen nyttige chat.completions.create - parametre: *max_tokens, temperature, top_p* og *n*   \n",
    "***temperature***: justerer \"kreativiteten\" i responsen: høyere temperatur gir mer fantasifulle og varierte svar, men også høyere forekomst av feil og hallusinasjoner. Default=1, max=2, min=0. For stabile og (ganske) strengt faktabaserte svar, velg *temperature*=0. Høye temperature-verdier kan innimellom gi lang responstid.  \n",
    "***top_p***: cutoff sannsinlighetsverdi for predikerte tokens. En lavere verdi innebærer et strengere \"filter\" for å legge til nye tokens i responsen og gir dermed kortere, mindre varierte og mer konsise svar. Default=1. \n",
    "Merk: openAI anbefaler å bare bruke en av parametrene *temperature* og *top_p* for å tune svarene, og la den andre beholde defaultverdien (=1).  \n",
    "***max_tokens***: grense for antall tokens brukt (i prompt og respons samlet).  \n",
    "***n***: antall responser returnert. I noen situasjoner kan det være hensikstsmessig å generere flere svar, og så velge det \"beste\" ut fra et eller flere vurderingskriterier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0150d2-49db-4fd3-8ca1-9ddff3494221",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# bruk av temperature eller top_p for å justere \"kreativitet\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    top_p=1\n",
    ")\n",
    "pprint(response.choices[0].message.content, width=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741015e2-f8c7-4c09-8d9d-9b3eb08dcfe6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# flere responser for samme forespørsel - illustrerer effekten av temperature\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    n=3\n",
    ")\n",
    "for c in response.choices:\n",
    "    pprint('Svar nr ' + str(c.index+1) + \": \"+  c.message.content, width=80)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c680e76c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# temperature=0 er ikke noen garanti mot hallusinasjoner\n",
    "# Justering prompt: You are very factual. If you do not know something say you do not know.\n",
    "\n",
    "messages = [  \n",
    "{\"role\": \"system\", \"content\": \"You are a tourist information agent in Oslo, Norway. \"},    \n",
    "{\"role\": \"user\", \"content\": \"Hvor finner jeg hovedkontoret til Sparebank 1 Utvikling?\"}  \n",
    "] \n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "pprint(response.choices[0].message.content, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a779baa0-a63c-4476-95fe-b87c2afba4a1",
   "metadata": {},
   "source": [
    "### 1.3 Flere potensielt nyttige chat.completions.create - parametre \n",
    "***frequency_penalty***: Reduserer sannsynligheten for å legge til tokens i forhold til hvor ofte de forekommer i den foregående teksen. Skala: -2 til 2, høyere positiv verdi indikerer strengere \"straff\" for høy forekomst. <br/>\n",
    "***presence_penalty***: Reduserer sannsynligheten for å legge til tokens som allerede forekommer i den foregående teksten. Skala: -2 til 2, høyere positiv verdi indikerer strengere straff for tidligere forekomst. <br/>\n",
    "***stop***: kan brukes til å la utvalgte ord eller ordkombinasjoner avslutte svaret, hvis de forekommer.<br/>\n",
    "***seed***: (beta-funksjonalitet) lar deg sette seed-verdi for å generere et deterministisk utvalg. NB - denne funksjonaliteten er i beta, og det er ingen garanti for at samplingen faktisk blir fullstendig deterministisk.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d474379-cdda-40e5-962f-ab42e2051dde",
   "metadata": {},
   "source": [
    "## 2. Chat completions - objektet (responsen du får når du kaller chat.completions.create) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9984f1a-5fae-48c9-ad64-c566e798db67",
   "metadata": {},
   "source": [
    "Chat completions - objektet har mange elementer, mange av dem inneholder imidlertid bare relativt uinteressant referanseinformasjon. Se eventuelt [OpenAI API-dokumentasjon](https://platform.openai.com/docs/api-reference/chat/object) for en generell og oppdatert oversikt.\n",
    "\n",
    "Her vil vi fokusere på listen *choices* og raskt nevne listen *usage* ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3b4f0-159b-4ff3-ad3a-71946b9d6eef",
   "metadata": {},
   "source": [
    "<img src=\"resources/chat_completion.png\" alt=\"OpenAI chatCompletion\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b42230-bbd3-411b-ad1f-0850904458b8",
   "metadata": {},
   "source": [
    "**usage** (liste) inneholder en opptelling av antall tokens brukt i prompt, respons og til sammen. Dette kan være nyttig informasjon for kostnadskontroll, og for å forstå situasjoner der begrensningene på antall tokens i kontekst gir problemer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc175ba-c24d-4eb4-85e5-1f5ce7f82081",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sjekk av \"usage\" \n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    top_p=1\n",
    ")\n",
    "pprint(dict(response.usage))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7937b242-92db-4cc3-afbf-29d0d8c69537",
   "metadata": {},
   "source": [
    "***choices*** - listen inneholder responsen(e) fra modellen.<br/> \n",
    "Hvert ***choices[index]***- listeelement inneholder igjen disse elementene: \n",
    " - finish_reason: indikerer hvorfor svaret ble avsluttet, og kan være nyttig å undersøke i feilsituasjoner\n",
    " - logprobs: gir sannsynlighetsverdier for hver token i det genererte svaret\n",
    " - message (liste): den genererte responsen, med tilleggsinformasjon om hvordan den har blitt generert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966eb294-c2a8-4bbf-af6e-0b3f2bd31b60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# innholdet i et element fra \"choices\" \n",
    "pprint(dict(response.choices[0]), depth=1, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631d28c-6225-4e3b-b27d-a6f854da4576",
   "metadata": {},
   "source": [
    "***choices[index].message*** - listen inneholder responsteksten i tillegg til informasjon om hvordan teksten ble generert: \n",
    "- role: \"rollen\" som gir svaret (som oftest \"assistant\") \n",
    "- content: selve den generert teksten \n",
    "- tool_calls: beskrivelse av eventuelle \"tool calls\" (se under) som har blitt brukt for å lage svaret  \n",
    "\n",
    "Merk at innholdet av \"messages\" fra kallet til chat.completions.create IKKE er inkludert i responsen, kun (nyeste) output fra modellen. Hvis du ønsker å beholde oversikt over en flerstegs dialog må du i utgangspunktet selv bygge opp og ta vare på historikken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f95c9-368f-432c-af1f-bee38f2e42a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# innholdet i \"messages\" for et \"choices\" - element\n",
    "pprint(dict(response.choices[0].message), depth=1, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ffbb28-1d6f-4868-9277-e90486530513",
   "metadata": {},
   "source": [
    "## 3. Tools og Tool Calls - ekstra superkrefter til språkmodellen \n",
    "ChatCompletions-APIet gjør det mulig å gi språkmodellen tilgang til \"tools\" - verktøy i form av egendefinerte funksjoner - som den kan bruke når den svarer på en forespørsel. Dette kan for eksempel brukes til å hente oppdatert informasjon fra nettet eller andre kilder. \n",
    "\n",
    "Språkmodellen får ikke kjøre kode eller kalle eksterne systemer på egen hånd, men får oppgitt formelle beskrivelser av tilgjengelige funksjoner (\"tools\") den kan bruke. Når den behandler en forespørsel vil så modellen selv bestemme seg for om den ønsker å bruke noen av de tilgjengelige \"tools\". Om den ønsker å bruke en funksjon, returnerer den et funksjonskall i det spesifiserte formatet. Å faktisk utføre funksjonskallet må håndteres i koden som kaller API-et.  \n",
    "\n",
    "Oppsummert kan man oppsummere fremgangsmåten slik: \n",
    "1. Gjør kall til språkmodellen med forespørsel fra bruker, og gi modellen en liste med tilgjengelige hjelpefunksjoner i \"tools\" parameteren\n",
    "2. Ut fra forespørselen og funksjonsbeskrivelsene, velger modellen om den vil bruke en eller flere hjelpefunksjoner. Om den vil bruke en funksjon, returnerer den et JSON-objekt som matcher funksjonsbeskrivelsen\n",
    "3. Sjekk om modellen har returnert et funksjonskall (sjekk message.tool_calls-verdien) og kall i så fall den aktuelle hjelpefunksjonen med parameterverdiene oppgitt av modellen.\n",
    "4. For å få et endelig svar på bruker-forespørselen, kall LLM - modellen på nytt med responsen fra (3) som et element i \"messages\" listen (med role=\"tool\") \n",
    "\n",
    "Et enkelt (og litt klossete) eksempel på denne teknikken vises under."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f21d74-9b84-48ca-ab17-930692e6cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# definerer en dummy-funksjon språkmodellen skal kunne kalle\n",
    "\n",
    "\n",
    "# Hardkodet værtjeneste - kan erstattes med eksternt API-kall eller lignende\n",
    "def get_current_weather(location):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"bergen\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Bergen\", \"temperature\": \"-10\", \"unit\": \"celcius\", \"weather\": \"heavy snow\"})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"85\", \"unit\": \"fahrenheit\", \"weather\": \"sunny\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\", \"weather\": \"cloudy\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "# json beskrivelse av værtjenesten med tekstbeskrivelse av funksjon (\"description\") og argumenter\n",
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc836cc-1690-4fd9-90a7-8832e01afe9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run_conversation: hjelpefunksjon for å besvare en bruker-forespørsel, potensielt med hjelp av en \"tool\"\n",
    "\n",
    "def run_conversation(query):\n",
    "    # Steg 1 : initier \"konversasjon\" - send forespørsel og beskrivelse av tilgjengelige hjelpefunksjoner til modell-API\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo-1106\",\n",
    "        messages=messages,\n",
    "        temperature=1.5,\n",
    "        tools=tools, # funksjonsbeskrivelse \n",
    "        tool_choice=\"auto\",  # auto er default, kan overstyres hvis man vil \"nekte\" modellen å gjøre valg\n",
    "    )\n",
    "\n",
    "    # Steg 2: ta vare på den initielle responsen, og sjekk om modellen ønsker å bruke hjelpefunksjon\n",
    "    response_message = response.choices[0].message\n",
    "    messages.append(response_message)\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    if tool_calls:\n",
    "        # Steg 3 (optional): kall til hjelpefunksjon(er)\n",
    "        # bare en funksjon i dette eksemplet, men man kan ha flere\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        } \n",
    "        \n",
    "        for tool_call in tool_calls:\n",
    "            # kall den valgte funksjonen med parameterverdiene modellen har spesifisert\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments) \n",
    "            function_response = function_to_call(\n",
    "                location=function_args.get(\"location\")\n",
    "            ) \n",
    "            \n",
    "            # utvid samtalen med respons fra hjelpefunksjonen\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            ) \n",
    "            \n",
    "        # Steg 4 (optional): send data om hvert funksjonskall og hver respons tilbake til modellen i et nytt kall.\n",
    "        # Få til slutt en endelig respons fra modellen gitt resultatet av funskjonkallene. \n",
    "        second_response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=messages,\n",
    "        )  \n",
    "        messages.append(second_response.choices[0].message)  \n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f72ff0-1ece-40a8-aede-d92fa1a0160d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# funksjonskall anvendt: \n",
    "\n",
    "# med \"run_conversation\" - hjelpefunksjonenkan vi se steg for steg hvordan modellen svarer på ulike forespørsler når den har tilgang på et verktøy. \n",
    "#  eksempler: vær i Bergen, band-medlemmer - Python lister...\n",
    "# Terskelen for å bruke verktøy ser ut til å være ganske lav.\n",
    "\n",
    "test_messages=run_conversation(\"Who were the founding members of New Order?\")\n",
    "\n",
    "for msg in test_messages: \n",
    "    pprint(dict(msg))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ad010-21f9-419a-b5dc-1904d7f73b0a",
   "metadata": {},
   "source": [
    "## 4. OpenAIs embeddings - endepunkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ace7a8-2c34-4d9a-ba0e-87dedc12c57f",
   "metadata": {},
   "source": [
    "<center><img src=\"resources/image resolution.jpg\" alt=\"image resolution\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d6754f-bc70-4a12-b732-20f2ddecefc4",
   "metadata": {},
   "source": [
    "OpenAI har et eget endepunkt for å konvertere tekst til såkalte embeddings - som er lange flyttalls - vektorer. Man kan tenke på embeddings som en dimensjonsreduseringsteknikk for tekstdata. Embeddings brukes typisk til å sammenligne tekstelementer i forhold til innholdsmessig likhet på en rask og beregningsmessig billig måte. \n",
    "\n",
    "I LLM - applikasjoner brukes ofte embeddings til å bygge relevant kontekst i prompts. Ved hjelp av embeddings og forskjellige numeriske sammenligningsmål (similarity measures) kan tekst fra store mengder bakgrunnsinformasjon sorteres etter relevans for en spesifikk bruker-forespørsel.\n",
    "\n",
    "Når man bruker OpenAIs embeddings - endepunkt, vil både maksimalt antall tokens i input-teksten og dimensjonaliteten til vektoren som returneres være avhenging av hvilken embeddings-modell som brukes. I eksemplene under bruker vi *text-embedding-3-small* - med denne kan input-teksten være inntil 8191 tokens, og vektorene som returneres er 1536-dimensjonale, dvs de inneholder alltid 1536 tall.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb2c22-5bcc-41dc-a47d-956e395d383d",
   "metadata": {},
   "source": [
    "### 4.1 Enkel bruk av embeddings\n",
    "I koden under leser vi inn noen eksempeltekster, og bruker embeddings til å sammenligne og sortere dem ut fra semantisk innhold. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360b444-be14-4d5c-b4f0-52f4b560d790",
   "metadata": {},
   "source": [
    "#### 4.1.1 Innlesning av eksempeldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91d554-fa95-4870-9ecb-892ed900bfa4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# leser inn et lite datasett med teksteksempler\n",
    "\n",
    "# hjelpefunksjon for å sjekke antall tokens i tekst\n",
    "def token_count(text, encoding_model):\n",
    "    encoding=tiktoken.get_encoding(encoding_model)\n",
    "    n_tokens = len(encoding.encode(text))\n",
    "    return n_tokens\n",
    "\n",
    "# leser tekster fra csv-fil, beregner antall tokens og inspiserer. \n",
    "df_text = pd.read_csv('data/text_samples_mat.csv', header=0, sep=';')\n",
    "df_text['n_tokens']=df_text.apply(lambda row: token_count(row['quote_text'], \"cl100k_base\"), axis=1)\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da793f6-c0f0-4426-a625-e41292834e7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Inspeksjon av enkelttekst \n",
    "print(df_text.loc[2,'quote_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ce634-e7b6-4d0e-b855-def23539662b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 4.1.2 Lage embeddings ved hjelp av OpenAI\n",
    "Det er relativt rett frem å lage embeddings med kall til *embeddings.create*. Her bruker vi en enkelt input-streng, men APIet aksepterer også et array av strenger som input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c520da-b3ff-4c05-9c26-8e18c8bb386a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lager hjelpefunksjon for å hente embeddings fra OpenAI\n",
    "\n",
    "# embedding hjelpefunksjon - returnerer embedding-objekt fra OpenAI laget med valgt modell\n",
    "def embed_helper(AIclient, text, model_name):\n",
    "    embedding=client.embeddings.create(\n",
    "          model=model_name,\n",
    "          input=text,\n",
    "          encoding_format=\"float\")\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9403ef28-5802-4b3f-8e11-4effe94396d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lager embedding med kall til API\n",
    "test_embedding=embed_helper(client, df_text.loc[2,'quote_text'], \"text-embedding-3-small\")\n",
    "\n",
    "# sjekker dimensjonalitet, inspiserer \"rå\" embedding output:\n",
    "print('No of elements: '+str(len(test_embedding.data[0].embedding))) \n",
    "print(test_embedding.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681851a-c785-4825-8d59-2e2ef1a3b91d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# enkel visualisering av embeddingvektor\n",
    "\n",
    "# lager hjelpefunksjon for visualisering\n",
    "def barplot_embedding(embedding, label_list):\n",
    "    sns.heatmap(np.array(embedding.data[0].embedding).reshape(-1,1536), cmap=\"Greys\", center=0, square=False, xticklabels=False, cbar=False)\n",
    "    plt.gcf().set_size_inches(13,1)\n",
    "    plt.yticks([0.5], labels=[label_list])\n",
    "    plt.show()\n",
    "\n",
    "# barplot for embedding av utvalgte tekster\n",
    "test_embedding=embed_helper(client, \"\", \"text-embedding-3-small\")\n",
    "barplot_embedding(test_embedding, \"empty string\")\n",
    "\n",
    "test_embedding=embed_helper(client, df_text.loc[0,'quote_text'], \"text-embedding-3-small\")\n",
    "barplot_embedding(test_embedding, df_text.loc[0,'title'])\n",
    "\n",
    "test_embedding=embed_helper(client, df_text.loc[1,'quote_text'], \"text-embedding-3-small\")\n",
    "barplot_embedding(test_embedding, df_text.loc[1,'title'])\n",
    "\n",
    "test_embedding=embed_helper(client, df_text.loc[3,'quote_text'], \"text-embedding-3-small\")\n",
    "barplot_embedding(test_embedding, df_text.loc[3,'title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf68758f-c6bd-4b13-97c2-029f026be20a",
   "metadata": {},
   "source": [
    "#### 4.1.3 Rangering av tekstelementer etter innholdsmessig likhet\n",
    "Med hjelp av et numerisk sammenligningsmål (similarity measure) - her er det flere tekniske muligheter - kan vi rangere sitatene i datasettet etter likhet med hverandre, eller etter likhet med en annen gitt tekst.\n",
    "\n",
    "Rent teknisk fungerer dette uavhengig av språk, men sammenligning av tekst fra ulike språk gir mindre pålitelige resultater."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd69061-2183-4b4e-9e15-c2d48544495b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Hjelpefunksjoner for tekstsammenligning\n",
    "\n",
    "# naiv hjelpefunksjon for å beregne cosinus similaritet mellom to tekststrenger\n",
    "def similarity_helper(AIclient, text_1, text_2, embed_model):\n",
    "    text_1_embedded_np=np.array(embed_helper(AIclient, text_1, embed_model).data[0].embedding).reshape(1,-1)\n",
    "    text_2_embedded_np=np.array(embed_helper(AIclient, text_2, embed_model).data[0].embedding).reshape(1,-1)\n",
    "    similarity=cosine_similarity(text_1_embedded_np, text_2_embedded_np)[0,0]\n",
    "    return similarity\n",
    "\n",
    "# hjelpefunksjon for å beregne similaritet mellom en variabel (df_text_column) i en DataFrame og en oppgitt tekst (input_text)\n",
    "def df_add_similarity(AIclient, df_text, df_text_column, input_text, embed_model):   \n",
    "    df_text['input_similarity']=df_text.apply(lambda row: similarity_helper(AIclient, row[df_text_column], input_text, embed_model), axis=1)\n",
    "    return df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1c3c44-b640-4f73-9099-9812c89112eb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# rangerer tekstelementer i en DataFrame ut fra similaritet med en input-tekst \n",
    "\n",
    "# definer input - tekst her (referanse for sammenligning):\n",
    "# input_text=\"Be, Act, Stay Curious!\"\n",
    "# input_text=\"Norias omsetning har eksplodert de siste årene, spesielt i Bergen.\"\n",
    "# input_text=\"Gourmetpølser har blitt vår nye nasjonalrett, jubler Pølsens Venner\"\n",
    "input_text=df_text.loc[1, 'quote_text']\n",
    "\n",
    "# Inkluderer input_tekst i datasett, for kontroll og som referanse \n",
    "df_input=pd.DataFrame(data={'quote_id':100, 'quote_name':'Input', 'author': 'user', 'title':'Input' , 'quote_text': input_text}, index=[10])\n",
    "df_total=pd.concat([df_text, df_input])\n",
    "\n",
    "# beregner similaritet\n",
    "df_add_similarity(client, df_total, 'quote_text', input_text, \"text-embedding-3-small\")\n",
    "\n",
    "# lager en enkel illustrasjon av tekst-similaritet \n",
    "values=list(df_total.sort_values('input_similarity', ascending=True)['input_similarity'])\n",
    "names=list(df_total.sort_values('input_similarity', ascending=True)[\"quote_text\"].str[:50])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.barh(names, values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bf571f-fb17-4653-ab35-191a1f98abbc",
   "metadata": {},
   "source": [
    "Prøv gjerne å laste opp egne data (lettest å gjenbruke struktur i .csv - fil), og gjør lignende tester med dem! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be6f32a-6052-4d5b-a4d4-47e40dfe0a77",
   "metadata": {},
   "source": [
    "#### 4.1.3 Gruppering og klassifisering av tekst\n",
    "\n",
    "Siden embeddings jo bare er vektorer med flyttall, er de teknisk sett enkle å plugge inn i forskjellige analysemetoder. En interessant mulighet er å  bruke ulike dimensjonsreduserings-teknikker til å gruppere og klassifisere tekster. \n",
    "\n",
    "Under gjør vi en prinsipalkomponent - analyse av eksempeldataene vi brukte over. Litt forenklet \"tvinger\" vi det 1536-dimensjonale rommet vi egentlig er i ned i færre dimensjoner, som er enklere for menneskehjerner å forholde seg til. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9de4d6-9191-452b-bd3b-b8f611fb0ae8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Lager nye hjelpefunksjoner for mer effektiv håndtering av embeddings\n",
    "# funksjon som for en gitt tekst (text) returnerer embedding-vektor som numpy-array \n",
    "def embed_helper_2(AIclient, text, model_name):\n",
    "    embedding=client.embeddings.create(\n",
    "          model=model_name,\n",
    "          input=text,\n",
    "          encoding_format=\"float\")\n",
    "    return np.array(embedding.data[0].embedding).reshape(1,-1)\n",
    "\n",
    "# hjelpefunksjon for å lagre embeddings, lables og eventuelle kategorier (cats) fra dataframe i numpy arrays.\n",
    "def embed_df(AIclient, df, text_col, label_col, cat_col, model_name): \n",
    "    embedding_array=df.apply(lambda row: embed_helper_2(AIclient, row[text_col], model_name)[0], axis=1, result_type=\"expand\")\n",
    "    labels=df[label_col]\n",
    "    cats=df[cat_col]\n",
    "    return np.asarray(embedding_array), np.asarray(labels),  np.asarray(cats)\n",
    "\n",
    "# similaritetsberegning for array\n",
    "def similarity_rank_2(AIclient, embed_array, input_text, embed_model): \n",
    "    input_text_embedded=embed_helper_2(AIclient, input_text, embed_model)\n",
    "    similarities=cosine_similarity(embed_array,input_text_embedded)\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b6be8-93db-4580-a44b-08d51fe39580",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Lager embeddings for dataframe fra 4.1.1 (df_text)\n",
    "embed_array, embed_lbl, embed_cat=embed_df(client, df_text, 'quote_text', 'title', 'author', \"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc6b719-9747-4151-b559-1b5208c5a6d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# prinsipalkomponent-analyse m plotting av 2 første komponenter\n",
    "\n",
    "def plot_embeddings(embedding_vectors, labels, cats, show_labels=False):\n",
    "    # Create a PCA model\n",
    "    pca_model = PCA(random_state=42)\n",
    "\n",
    "    # Fit and transform the data to obtain PCA coordinates\n",
    "    pca_result = pca_model.fit(embedding_vectors)\n",
    "    \n",
    "    #print(\"PCA 2-components explained variance:\" + str(pca_result.explained_variance_ratio_) + \"\\n\")\n",
    "    pca_trans = pca_model.fit_transform(embedding_vectors)\n",
    "\n",
    "    # Plot the PCA results\n",
    "    # explained variance\n",
    "    fig=plt.figure(figsize=(8, 12))\n",
    "    \n",
    "    ax=fig.add_subplot(211)\n",
    "    plt.title('PCA Explained variance by no. of components')\n",
    "    plt.plot(np.cumsum(pca_result.explained_variance_ratio_))\n",
    "    plt.grid(alpha=0.2)\n",
    "\n",
    "     # scatterplot by artist\n",
    "    ax=fig.add_subplot(212)\n",
    "    plt.title('PCA Projection of Embedding Vectors')\n",
    "    \n",
    "    cat_list=list(set(cats))\n",
    "    cat_vals=np.asarray([cat_list.index(c) for c in cats])\n",
    "\n",
    "    cmap='tab20'\n",
    "    color_map = plt.colormaps[cmap].resampled(20)\n",
    "    \n",
    "    for i, cat in enumerate(cat_list):\n",
    "        filter_arr = []\n",
    "        for catval in cats:\n",
    "            if catval==cat:\n",
    "                filter_arr.append(True)\n",
    "            else:\n",
    "                filter_arr.append(False)\n",
    "\n",
    "        plt.scatter(pca_trans[filter_arr, 0], pca_trans[filter_arr, 1], color=color_map(i/len(cat_list)), label=cat, s=20) \n",
    "       \n",
    "    if show_labels:\n",
    "        for i, lbl in enumerate(labels):\n",
    "            ax.text(pca_trans[:, 0][i],pca_trans[:, 1]  [i], labels[i], fontsize=8)         \n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.xlabel('PCA component 1')\n",
    "    plt.ylabel('PCA component 2')\n",
    "    plt.show()    \n",
    "    \n",
    "plot_embeddings(embed_array, embed_lbl, embed_cat, show_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be2dac-b509-4269-845e-55f63de9c7d6",
   "metadata": {},
   "source": [
    "### 4.2 Analyse av et større tekstdatasett\n",
    "\n",
    "Filen \"lyrics.csv\" inneholder rundt 25 000 sangtekster med kjente, kjære og mindre kjære artister. Under tester vi noen av de samme teknikkene på disse dataene. \n",
    "\n",
    "Først leser vi inn hele datasettet og lager etpar hjelpevariable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd28866f-fac0-4614-8bb2-e38918fc0dfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# innlesning av sangtekst datasett (25 000 sanger)\n",
    "df_sang = pd.read_csv('data/lyrics.csv', header=0, sep=',')\n",
    "\n",
    "# hjelpevariable for litt enklere oppslag og navigasjon\n",
    "df_sang['n_tokens']=df_sang.apply(lambda row: token_count(row['lyrics'], \"cl100k_base\"), axis=1)\n",
    "df_sang['song_id']=df_sang.index\n",
    "df_sang['artist']=df_sang['artist'].apply(lambda x: x[:-7]) # fjerner unødvendig \"Lyrics\" i artistnavn \n",
    "df_sang['song_label']=df_sang['song_id'].apply(str) + \" - \" + df_sang['artist'] + \" - \" +df_sang['song_name']\n",
    "\n",
    "# rask inspeksjon\n",
    "df_sang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c849fc-6b97-42f6-b8b1-cdfbde0c6e90",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sjekk av artister - viser de 200 med flest sanger (kan justeres)\n",
    "pd.options.display.max_rows=200\n",
    "df_sang.value_counts('artist', ascending=False)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50621624-08f1-4599-89c1-5c8eeb6e5839",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sjekk sanger for en valgt artist\n",
    "df_sang[df_sang['artist']=='Snoop Dogg'][:200]['song_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d42844-0b2f-49fc-91ac-ae0c79f19a17",
   "metadata": {},
   "source": [
    "For å begrense ventetid lager vi et tilfeldig sample på 100 sanger (kan justeres) som vi bruker videre. I koden under er samplet begrenset til noen utvalgte artister - gjør gjerne endringer i utvalget!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfffedc7-1585-4281-b7cc-67add68d5385",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lager sample på 100 sanger, noen valgte artister\n",
    "df_sang_sample=df_sang[df_sang['artist'].isin(['Bob Dylan', 'Snoop Dogg', 'Backstreet Boys', 'Lana Del Rey', 'Eminem', 'Taylor Swift'])].sample(n=100).reset_index()\n",
    "print(df_sang_sample['song_label'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf692d1-a639-47ed-a810-e81ce3fc5817",
   "metadata": {},
   "source": [
    "Vi lager så embeddings av tekstene i utvalget vårt med hjelp av funksjonene vi lagde i forrige avsnitt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f9ce3-7947-481a-b193-10381aa77445",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# lager array med embeddings\n",
    "sang_embed_array, sang_embed_lbl, sang_embed_cat=embed_df(client, df_sang_sample, 'lyrics', 'song_label', 'artist', \"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8835e5-a17a-44c7-9b68-bd7c8798ff46",
   "metadata": {},
   "source": [
    "Vi kan nå sjekke similaritet mellom tekstene, eller med annen input - tekst om vi ønsker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e5c99-63a0-4abc-9207-03246de0b4b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# beregning av similaritet med input-tekst\n",
    "\n",
    "#ref_lyrics: sangteksten vi ønsker å sammenligne de andre med. Her kan man selvfølgelig også oppgi en annen, vilkårlig tekst \n",
    "#ref_lyrics=df_sang_sample.iloc[21][\"lyrics\"]\n",
    "#ref_lyrics=\"Wholehearted, responsible, likable and competent\"\n",
    "#ref_lyrics=\"Christmas\"\n",
    "\n",
    "similarities=similarity_rank_2(client, sang_embed_array, ref_lyrics, 'text-embedding-3-small')\n",
    "df_sang_sample[\"similarities_arr\"]=similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef73645-c9b2-4e9d-9c0a-7cefe5b3fadc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# viser topp n og laveste n tekster, rangert etter similaritet med input_tekst\n",
    "n=5\n",
    "sim_col=\"similarities_arr\"\n",
    "\n",
    "values_topn=list(df_sang_sample.nlargest(n,sim_col).sort_values(sim_col, ascending=True)[sim_col]) \n",
    "values_smalln=list(df_sang_sample.nsmallest(n,sim_col).sort_values(sim_col, ascending=True)[sim_col])\n",
    "\n",
    "names_topn=list(df_sang_sample.nlargest(n,sim_col).sort_values(sim_col, ascending=True)[\"song_label\"])\n",
    "names_smalln=list(df_sang_sample.nsmallest(n,sim_col).sort_values(sim_col, ascending=True)[\"song_label\"])\n",
    "\n",
    "values=values_smalln+values_topn\n",
    "names=[c.replace('$', 'S') for c in names_smalln+names_topn] # Paid da cost to be da bo$$ quick fix  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "ax.barh(names_smalln, values_smalln)\n",
    "ax.barh(names_topn, values_topn)\n",
    "\n",
    "plt.title('Top/bottom similarity with ' + ref_lyrics[:50] + '...')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24deb568-3d3d-4bd2-b7b1-dcdf069b8092",
   "metadata": {},
   "source": [
    "Er det noen fornuft i rangeringen? Bruk gjerne hjelpefunksjonen under for å inspisere to tekster ved siden av hverandre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9bc393-6122-43d0-9b8c-509d6ed2868f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# hjelpefunksjon for sjekk av tekster mot hverandre\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def view_lyrics(df, song_id_1, song_id_2):\n",
    "    df_temp=df_sang_sample.loc[df_sang_sample['song_id'] == song_id_1].reset_index()\n",
    "    label1=df_temp.loc[0, \"song_label\"]\n",
    "    lyrics1=df_temp.loc[0, \"lyrics\"]\n",
    "\n",
    "    df_temp=df_sang_sample.loc[df_sang_sample['song_id'] == song_id_2].reset_index()\n",
    "    label2=df_temp.loc[0, \"song_label\"]\n",
    "    lyrics2=df_temp.loc[0, \"lyrics\"]\n",
    "    \n",
    "    html_code = f\"\"\"\n",
    "    <div style=\"display: flex; justify-content: space-between;\">\n",
    "        <div style=\"flex: 1; padding-right: 8px;\">\n",
    "            <h2>{label1}</h2>\n",
    "            <pre style=\"font-size: 9px;\"> {lyrics1} </pre>\n",
    "        </div>\n",
    "        <div style=\"flex: 1; padding-left: px;\">\n",
    "            <h2>{label2}</h2>\n",
    "            <pre style=\"font-size: 9px;\">{lyrics2}</pre>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    return html_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd0918-a5e9-4108-a621-cd6d42238bdb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# vis to tekster ved siden av hverandre - bruk sang_id for valg av tekster\n",
    "display(HTML(view_lyrics(df_sang_sample, 23312, 10172)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe5e71f-9528-4464-b529-1f4cfc3318c1",
   "metadata": {},
   "source": [
    "Vi kan også sjekke hvordan PCA eller t-SNE (t-distributed stochastic neighbor embedding) dimensjonsreduksjon plasserer sangene i to dimensjoner. Ser det f eks ut til at sangene fra samme artist er samlet i noen grad?\n",
    "\n",
    "For TSNE, test gjerne effekten av å justere på perplexity - parameteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14575c46-87e4-436e-92d2-3e1a0442ae63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PCA-plott av embeddings, med artist som kategori\n",
    " \n",
    "plot_embeddings(sang_embed_array, sang_embed_lbl, sang_embed_cat, show_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb503c27-ba68-4e3e-8ee9-cbc6ad8d3bcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# t-SNE - alternativ (ikke-lineær) metode for dimensjonsreduksjon\n",
    "\n",
    "def plot_embeddings_tsne(embedding_vectors, labels, cats, show_labels=False, perplexity=10):\n",
    "    # Create a TSNE model\n",
    "    tsne_model = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "\n",
    "    # Fit and transform the data to obtain PCA coordinates\n",
    "    tsne_result = tsne_model.fit_transform(embedding_vectors)\n",
    "\n",
    "    # Plot the TSNE result\n",
    "    cat_list=list(set(cats))\n",
    "    cat_vals=[cat_list.index(c) for c in cats]\n",
    "    fig=plt.figure(figsize=(8, 6))\n",
    "    ax=fig.add_subplot(111)\n",
    "\n",
    "    plt.title('PCA Projection of Embedding Vectors')\n",
    "    \n",
    "    cmap='tab20'\n",
    "    color_map = plt.colormaps[cmap].resampled(20)\n",
    "    \n",
    "    # scatterplot by artist\n",
    "    for i, cat in enumerate(cat_list):\n",
    "        filter_arr = []\n",
    "        for catval in cats:\n",
    "            if catval==cat:\n",
    "                filter_arr.append(True)\n",
    "            else:\n",
    "                filter_arr.append(False)\n",
    "\n",
    "        plt.scatter(tsne_result[filter_arr, 0], tsne_result[filter_arr, 1], color=color_map(i/len(cat_list)), label=cat, s=20) \n",
    "                 \n",
    "    if show_labels:\n",
    "        for i, lbl in enumerate(labels):\n",
    "            ax.text(tsne_result[:, 0][i],tsne_result[:, 1]  [i], labels[i], fontsize=8)   \n",
    "            \n",
    "    plt.legend()  \n",
    "    plt.title('TSNE Projection of Embedding Vectors')\n",
    "    \n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.show()    \n",
    "    \n",
    "# test gjerne forskjellige verdier for perplexity\n",
    "plot_embeddings_tsne(sang_embed_array, sang_embed_lbl, sang_embed_cat, perplexity=10, show_labels=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_kurs",
   "language": "python",
   "name": "rag_kurs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
